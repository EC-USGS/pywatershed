{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec384e1a-f8d9-455b-a59a-a484a9c3fe00",
   "metadata": {},
   "source": [
    "# 01 - Automated Testing\n",
    "\n",
    "The pynhm automated testing is the basis for continuous integration (CI). Coupled with good coverage, CI allows for peace of mind and more rapid and robust code development. The tests themselves also provide a window into how to use the code base in many cases. \n",
    "\n",
    "However, the main reason to start with testing as the first notebook (after establishing the pynhm environment) is that the test data are used as input to many of the examples that will follow. This notebook gives a quick overview of generating the test data and running the pynhm tests but does not go into detail on the contents of the tests. \n",
    "\n",
    "Automated testing is typically performed on the command line as shown here (though it could be done from within python) and this notebook is to be run in the bash kernel in the pyws_nb conda environment installed in notebook 00.\n",
    "\n",
    "The automated testing uses pytest. Pytest is an executable called from the command line which has many of its own options. Bear in mind that you can see the full listing of options by typing `pytest --help`. I will highlight several of these here that we will use below.\n",
    "\n",
    "\n",
    "```\n",
    "pytest --help\n",
    "\n",
    "...\n",
    "  --pdb                 start the interactive Python debugger on errors or KeyboardInterrupt.\n",
    "  \n",
    "...\n",
    "  \n",
    "  --capture=method      per-test capturing method: one of fd|sys|no|tee-sys.\n",
    "  -s                    shortcut for --capture=no.\n",
    "  \n",
    "...\n",
    "  \n",
    "  -v, --verbose         increase verbosity.  \n",
    "  \n",
    "...\n",
    "\n",
    "  -n numprocesses, --numprocesses=numprocesses\n",
    "                        Shortcut for '--dist=load --tx=NUM*popen'. With 'auto', attempt to detect physical CPU\n",
    "                        count. With 'logical', detect logical CPU count. If physical CPU count cannot be found,\n",
    "                        falls back to logical count. This will be 0 when used with --pdb.\n",
    "```\n",
    "\n",
    "Pytest generally likes to suppress output to the terminal and keep reporting to a minimum. The assumption is that typically every tests passes. It will report what tests fail at the end of the test and those can be run individually with terminal output (`-s`), increased pytest verbosity (`-v`) and even interactive debugging (`--pdb`). The option to parallelize the tests it helpful as it can dramatically reduce wait time (`-n=auto` or `-n=4`). \n",
    "\n",
    "## Requirements: pyws_nb virtual env\n",
    "The pynhm virtual environment was installed in notebook 00. You need this environment to proceed. __This notebook is to be run with a python kernel using the conda env: pyws_nb.__ This means we'll pass python variables to bash cell magics below, but that seemed to be the most portable solution (on Windows).\n",
    "\n",
    "\n",
    "## pynhm_root variable\n",
    "Define the location of the pynhm repository. This should be the location you defined in notebook 00. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebef425b-174b-4c16-96cc-ba576584126d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext jupyter_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5508df6b-a23d-4d8f-a585-ee5452f51661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pywatershed\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import pathlib as pl\n",
    "\n",
    "# set up paths for notebook\n",
    "# this is as pywatershed is installed\n",
    "pynhm_repo_root = pywatershed.constants.__pywatershed_root__.parent\n",
    "pynhm_test_data_dir = pynhm_repo_root / \"test_data\"\n",
    "pynhm_test_data_scripts_dir = pynhm_repo_root / \"test_data/scripts\"\n",
    "pynhm_autotest_dir = pynhm_repo_root / \"autotest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be00ecc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_subprocess(args=None, cwd=None, check=False, print_output=True):\n",
    "    \"\"\"Helper function for system commands.\"\"\"\n",
    "    process = subprocess.run(\n",
    "        args,\n",
    "        check=check,\n",
    "        cwd=cwd,\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.STDOUT,\n",
    "    )\n",
    "    if print_output:\n",
    "        print(process.stdout.decode(\"utf8\"))\n",
    "    return process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c260088d-1982-485f-9257-b24bf3a8a101",
   "metadata": {},
   "source": [
    "## Run PRMS to generate test answers and pynhm inputs\n",
    "\n",
    "By default, the \"tests\" which run PRMS to generate answers and inputs for pynhm run for all 3 test domains, unless otherwise specified. One can actually see options (specific to this conftest.py) in `pytest --help` output, under \"custom options\" as will be shown later in this notebook (for the pynhm tests).\n",
    "\n",
    "The three test domains have their basic data in these folders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9011edfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for domain in [\"hru_1\", \"drb_2yr\", \"ucb_2yr\"]:\n",
    "    print(f\"\\n\\nDirectory: {domain}\")\n",
    "    dirname = pynhm_test_data_dir / domain\n",
    "    for entry in dirname.iterdir():\n",
    "        if entry.is_file():\n",
    "            print(entry.name, end=\"  \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69a70b2-9f81-48a4-b717-3841b55a6654",
   "metadata": {},
   "source": [
    "If your repository is not freshly cloned, the above results may not look the same as other files have already been generated (as we will generate below).\n",
    "\n",
    "Note that on Windows, symbolic links (symlinks) generally require administrator access to set up. We don't want to have to maintain multiple copies of the same file, so as a workaround, we'll copy the files that *are* updated (from 'common') to the domain folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b4348b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the master 'single_hru' control file to control.test\n",
    "for domain in [\"hru_1\"]:\n",
    "    src = pynhm_test_data_dir / \"common/control.single_hru\"\n",
    "    dst = pynhm_test_data_dir / f\"{domain}/control.test\"\n",
    "    try:\n",
    "        shutil.copy(src, dst)\n",
    "    except shutil.SameFileError:\n",
    "        pass\n",
    "\n",
    "# copy the master 'multi_hru' control file to control.test\n",
    "for domain in [\"drb_2yr\", \"ucb_2yr\"]:\n",
    "    src = pynhm_test_data_dir / \"common/control.multi_hru\"\n",
    "    dst = pynhm_test_data_dir / f\"{domain}/control.test\"\n",
    "    try:\n",
    "        shutil.copy(src, dst)\n",
    "    except shutil.SameFileError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ac2341-8b1e-487f-b58d-3d51ce8c5ed3",
   "metadata": {},
   "source": [
    "The files listed above in each domain directory represent the data needed to run PRMS in an NHM configuration on each of the domains for 2 years in the case of the Delaware River and the Upper Colorado Basins. The inputs for hru_1 allow a 40 year run on a single HRU. More details about these domains will be provided in subsequent notebooks. \n",
    "\n",
    "Now we will run PRMS for each of these domains and generate output in an `output/` subdirectory of each domain directory listed above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ca040a",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = [\n",
    "    \"pytest\",\n",
    "    \"-n=4\",\n",
    "    \"test_run_domains.py\",\n",
    "]\n",
    "_ = run_subprocess(\n",
    "    args=args,\n",
    "    cwd=pynhm_test_data_scripts_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6936ae-94a1-4c87-99b5-7b9a3bf105ac",
   "metadata": {},
   "source": [
    "## Convert PRMS outputs to netcdf\n",
    "\n",
    "PRMS generates CSV output files. For example, for the DRB the file listing is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a639575",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint as pp\n",
    "\n",
    "dirname = pynhm_test_data_dir / \"drb_2yr/output\"\n",
    "filelist = []\n",
    "for entry in dirname.iterdir():\n",
    "    if entry.is_file():\n",
    "        filelist.append(entry.name)\n",
    "pp(filelist, compact=True)\n",
    "print(f\"\\n Number of files: {len(filelist)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632685c7-070a-4493-9c53-f1145eb954aa",
   "metadata": {},
   "source": [
    "We convert these files to netcdf and generate a hand full of extra, derivative files as well in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7396b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = [\n",
    "    \"pytest\",\n",
    "    \"-n=4\",\n",
    "    \"test_nc_domains.py\",\n",
    "]\n",
    "_ = run_subprocess(\n",
    "    args=args,\n",
    "    cwd=pynhm_test_data_scripts_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386decb6-9a56-47e3-9255-978164225c12",
   "metadata": {},
   "source": [
    "These netcdf files are the results of running PRMS 5.2.1. These files are used for evaluating the results/simulations of pynhm and also as inputs to individual process models (e.g. PRMSRunoff) in pywatershed. Netcdf files can be inspected on the command line with the ncdump utility. Though it's installed with the pyws_nb environment, specifying the path to ncdump is a pain here. Instead, we'll display the datasets from xarray, which is very similar to `ncdump -h`. In the highlevel metadata shown, note that the time durations and number of HRUs are evident by looking at the surface runoff variable (sroff) for each domain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3582778-71d5-4369-b663-e9dbf703d995",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "for domain in [\"hru_1\", \"drb_2yr\", \"ucb_2yr\"]:\n",
    "    print(domain)\n",
    "    display(\n",
    "        xr.open_dataset(\n",
    "            pl.Path(f\"{pynhm_repo_root}/test_data/{domain}/output/sroff.nc\")\n",
    "        )\n",
    "    )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53008603-cb49-418c-80b2-fd716c4ba45d",
   "metadata": {},
   "source": [
    "## pynhm autotest\n",
    "Now we can run the suite of pynhm tests, as we just genereated all the answers and input data. This verifies that your pynhm code base and your virtual environment are copacetic (assuming the commit being tested passed CI). First, I will point out that `pytest --help` even returns options for the test in the current directory under \"custom options\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fde847d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ll in stout.splitlines():\n",
    "    if cc > 7:\n",
    "        cc = 0\n",
    "    if \"Custom\" in str(ll) or (cc > 0 and cc < 7):\n",
    "        cc += 1\n",
    "        print(ll.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29ca2f5-4ed3-4110-b229-7824fca0a7d1",
   "metadata": {},
   "source": [
    "Now we'll run all pynhm tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a23c46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = [\"pytest\", \"-n=4\", \"--all_domains\"]\n",
    "_ = run_subprocess(args, cwd=pynhm_autotest_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27086c0-cc72-4b46-bf10-a6a2fb4e155e",
   "metadata": {},
   "source": [
    "We see that some tests are marked \"x\" for \"expected failure\". Some of these fail (x) and some pass (X) as the expected failures are typically just for one of the three domains. We also see generated warnings and the time taken. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "bbe09fef76cf7230519a15f5b5db67b9bc006ff3be45d40b7521af6d93d5409f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
