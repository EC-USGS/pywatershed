{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08f842ce-586b-490c-9e9e-97321918e5b8",
   "metadata": {},
   "source": [
    "# Building process models in pywatershed\n",
    "\n",
    "*James McCreight, USGS/UCAR*\n",
    "\n",
    "## Introduction \n",
    "\n",
    "It is a major scientific challenge to simulate and predict water quantity and quality in the environment. Over the years, many analytical, numerical, and statistical approaches have been developed to address this problem. The choice of modeling approaches often depends on specific real-world problems, resulting in a wide variation of models across different applications.\n",
    "\n",
    "The United States Geological Survey (USGS), among others, has an important legacy of models for simulating water quantity and quality in the environment. The USGS is also building new models for emerging applications. To enhance their effectiveness, it is crucial to establish interoperability of existing and future models. The Enterprise Capacity (EC) project at the USGS aims to create flexible, reusable, and interoperable models that can support water quantity and quality modeling across a broad range of applications.\n",
    "\n",
    "Focusing on water quantity, this notebook highlights on-going development to redesign core USGS hydrologic simulation capabilities into a modular and interoperable Python package. The goal of this package is to support flexible representations of conceptual hydrologic process models and the hypothesis testing of model suitability for a given application. \n",
    "\n",
    "The USGS National Hydrologic Model (NHM, Regan et al., 2018) is a specific, national-scale instance of physical process models within the Precipitation-Runoff Modeling System (PRMS, Regan et al., 2015). The PRMS concepts used in the NHM have been expressed in the Python package `pywatershed`. In this notebook, we demonstrate modeling of the Delaware River Basin (DRB) subdomain of the NHM for a 2 year period using `pywatershed`.\n",
    "\n",
    "The goal of this notebook is to demonstrate the use of `pywatershed` for interested users, highlighting its modularity and its self-describing nature. The outline of this notebook is as follows:\n",
    "\n",
    "\n",
    "* Results\n",
    "    - *Requirements and prerequisites:* What is needed to get up and running. \n",
    "    - *NHM in pywatershed:* We from atmospheric forcings/inputs and show how the model is designed at a conceptual process level\n",
    "    - *Submodels in pywatershed:* We demonstrate modularity by isolating and executing a submodel of the full NHM model.\n",
    "    - *Zooming in:* We delve into greater detial on how to query and control the model and its execution. \n",
    "* Conclusions\n",
    "\n",
    "\n",
    "## Results\n",
    "\n",
    "### Requirements:\n",
    "The conda/mamba environment specified in `env/pws-env.yml` is what is required to run this notebook. The `README.md` file describes setting up the environment, including using frozen versions of the environment also include in the `env/` directory.\n",
    "\n",
    "We'll use `jupyter_black` to keep our minds off formatting our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7275fe95-8e0b-45f9-837c-ad6f7d38ced7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext jupyter_black"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033f5af8-4d3e-4e25-af7f-beec712d5ba1",
   "metadata": {},
   "source": [
    "Our python imports for this notebook follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816e410d-c806-4467-8d56-e6add8c7f516",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import platform\n",
    "import pathlib as pl\n",
    "from platform import processor\n",
    "from pprint import pprint\n",
    "import urllib\n",
    "import zipfile\n",
    "\n",
    "import hvplot.pandas  # noqa\n",
    "import pywatershed as pws\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11df1c93-b47a-49b1-afd9-314022e60bc2",
   "metadata": {},
   "source": [
    "The directory which contains all the input/output for this notebook, we will call the `root_dir`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4950eb10-25c0-403c-80ef-857cd003bc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkg_root_dir = pws.constants.__pywatershed_root__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b024b73-cd4e-4a6c-9e8b-77ccfd525290",
   "metadata": {},
   "source": [
    "Well grab the GIS files for our domain from an earlier github release artifact (the repository was previously called `pynhm` when its scope was just the replicating the NHM). This only executes if the files are not already local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35632757-889d-4271-930a-6904b8590b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "gis_dir = pkg_root_dir / \"data/pywatershed_gis\"\n",
    "if not gis_dir.exists():\n",
    "    gis_url = \"https://github.com/EC-USGS/pywatershed/releases/download/v2022.0.1/pynhm_gis.zip\"\n",
    "    gis_file = pkg_root_dir / \"pynhm_gis.zip\"\n",
    "    urllib.request.urlretrieve(gis_url, gis_file)\n",
    "\n",
    "    with zipfile.ZipFile(gis_file, \"r\") as zz:\n",
    "        zz.extractall(pkg_root_dir)\n",
    "\n",
    "    (pkg_root_dir / \"pynhm_gis\").rename(gis_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19fa3d3-60b6-4c0e-9b12-817ce4daeda1",
   "metadata": {},
   "source": [
    "### Full NHM configuration for the Delaware River Basin\n",
    "\n",
    "Now we setup the full NHM model (from atmosphere forcing files through stream channel flow), on the DRB subdomain of the NHM. We will see the details of this model shortly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c460bc54-bba5-4493-9d1f-e2fe6672b186",
   "metadata": {},
   "source": [
    "Specify where our domain and GIS files for the Delaware River Basin 2-year run are located:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416714c2-0e99-4b92-ad7e-0f1dddfa8998",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_dir = pkg_root_dir / \"data/drb_2yr\"\n",
    "domain_gis_dir = gis_dir / \"drb_2yr\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d9f6cf-5170-49ff-94a0-0adfe978d103",
   "metadata": {},
   "source": [
    "The atmospheric forcing files for this model are in the `domain_dir`. The NHM input files als include its parameter and control files, also included in the `domain_dir`. The files being loaded here are native NHM files though these can be re-expressed in other formats. In `pywatershed` we use all of the parameter file necessary to support the model. We do not make extensive use of the control file, but do use parts of it. The Control object/class, manages not only the control data but also the parameter object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2488f925-fac2-4911-b474-2cb3c9b012b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = pws.parameters.PrmsParameters.load(domain_dir / \"myparam.param\")\n",
    "control = pws.Control.load(domain_dir / \"control.test\")\n",
    "control.edit_n_time_steps(90)\n",
    "control.config = control.config | {\n",
    "    \"input_dir\": domain_dir,\n",
    "    \"budget_type\": \"warn\",\n",
    "    \"calc_method\": \"numba\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6ec75f-485f-4cbe-9f54-9e365e4ceb9d",
   "metadata": {},
   "source": [
    "Specify where we'll write model output files for the full:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1318266-b6c1-4ab4-ac53-27c2a75fee96",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_root_dir = pl.Path(\"./01_process_models_output/nhm\")\n",
    "nhm_output_dir = output_root_dir / \"nhm\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad0a09c-c504-42d9-b07c-ba21164b7b7f",
   "metadata": {},
   "source": [
    "With all the requisite IO above, we can initialize the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b2ad91-9e4a-4b25-9510-d9d927f0acc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nhm = pws.Model(\n",
    "    [\n",
    "        pws.PRMSSolarGeometry,\n",
    "        pws.PRMSAtmosphere,\n",
    "        pws.PRMSCanopy,\n",
    "        pws.PRMSSnow,\n",
    "        pws.PRMSRunoff,\n",
    "        pws.PRMSSoilzone,\n",
    "        pws.PRMSGroundwater,\n",
    "        pws.PRMSChannel,\n",
    "    ],\n",
    "    control=control,\n",
    "    parameters=params,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c69fc4-34c3-48bf-948f-0df7c5769c78",
   "metadata": {},
   "source": [
    "The individual processes listed, from `PRMSSolarGeometry` through `PRMSChannel`, are one-way coupled in the order shown from top to bottom. This model initialization clearly indicates the working model conceptual at a high-level. We will see these processes in more fine-grained detail shortly.\n",
    "\n",
    "The control and directory to scan for input are passed. We ask for a warning if mass budgets do not balance. Finally, we request the `numba` calculation method. The `numba` Python package is a just-in-time compiler that can take the code written using the `numpy` package and accelerate it by compiling at run time. More on this below.\n",
    "\n",
    "Before running, we request NetCDF output to the desired output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3d54f5-84c0-42f4-9186-24e35d017dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "nhm.initialize_netcdf(output_dir=nhm_output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88b494c-c39c-4653-b3c6-f8372606f26f",
   "metadata": {},
   "source": [
    "We'll time our run and ask the model to finalize at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33866b9-d490-4b80-b8ee-2f02eb86525b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "nhm.run(finalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19d528e-01f0-4cf0-855e-eea532672836",
   "metadata": {},
   "source": [
    "Several things are happening, as can be seen in the output produced. When the model starts, we see a pause while the message \"X jit compiling with numba\" is printed. For each process listed, this is when the numba just-in-time (jit) compiler compiles the code for the process representation. The processes that benefit from jit compiling are challenging to vectorize and have a loop over space which is accelerated by the compiling. The remaining processes, not jit compiled, are all vectorized: PRMSSolarGeometry, PRMSAtmosphere. In fact, because these two processes are strictly pre-processing between input files and the rest of the model (all their input is know in advance), they are both vectorized in space and time. This is why all the variables are written out before the rest of the model runs.  \n",
    "\n",
    "A note on PRMSGroundwater is that it is both vectorized and jit-compiled. The numba, jit-compiled code is only slightly slower so it has not been removed.\n",
    "\n",
    "Now that we've run our NHM model on the DRB, let's get a flavor for the \"ultimate\" variable simulated on the domain: streamflow. Still in memory are the streamflow values from the final timestep, we'll plot those on the stream network overlaid on the watershed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7a3a1d-4f98-4f86-a25b-c53e44783b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_plot = pws.analysis.ProcessPlot(domain_gis_dir)\n",
    "proc_name = \"PRMSChannel\"\n",
    "var_name = \"seg_outflow\"\n",
    "proc = nhm.processes[proc_name]\n",
    "display(proc_plot.plot(var_name, proc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83052466-f75f-41e0-9b6d-4982dbe2e338",
   "metadata": {},
   "source": [
    "Above we see the spatial domain, the outline of its extent, the network on which streamflow is calculated, and the final simulated values of streamflow. \n",
    "\n",
    "Let us turn towards the model structure in more detail: how do atmospheric inputs/forcings result in the simulated streamflow above? We will produce the model graph which shows the flow of information from the input files through all the process representations, all the way down to the channel streamflow. First we print a color legend for each represented process in the NHM. Each process is outlined by a box of this color and values/fluxes flowing from a process have the color of the originating process. Finally, a variable outlined (above and on the sides) participates in the mass budget of its process. This diagram gives some very specific information of the model conceptualization, how the processes relate to each other, and the complexity of the indivdual processes. (Note that the underlying graphviz/dot program that generates the plot is not fully working on Mac ARM/M1, so plots here and below are less detailed if you are are using such a machine, the notebooks in the repo will be complete for your reference.) Each process's data is placed into one of three categories: inputs(blue), parameters(orange), and variables(green). All of this information is public for each process (indeed in static methods) so we can produce these plots programatically without needing to run the Model. The Model object contains all the information needed to generate the plot when it is initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95574d33-f8ff-42f4-ae9b-316dcc152a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = pws.analysis.utils.colorbrewer.nhm_process_colors(nhm)\n",
    "pws.analysis.utils.colorbrewer.jupyter_palette(palette)\n",
    "show_params = not (platform == \"darwin\" and processor() == \"arm\")\n",
    "try:\n",
    "    pws.analysis.ModelGraph(\n",
    "        nhm,\n",
    "        hide_variables=False,\n",
    "        process_colors=palette,\n",
    "        show_params=show_params,\n",
    "    ).SVG(verbose=True, dpi=55)\n",
    "except:\n",
    "    print(\"In some cases, dot fails on Mac ARM machines.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bf8b0f-5197-4467-8e2d-3c98060f8e68",
   "metadata": {},
   "source": [
    "### NHM Submodel for the Delaware River Basin \n",
    "Suppose you wanted to change parameters or model process representation in the PRMSSoilzone to better predict streamflow. As the model is 1-way coupled, you can simply run a submodel starting with PRMSSoilzone and running through PRMSChannel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f20704-36e4-407b-a7a8-1964563cf79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "submodel_processes = [pws.PRMSSoilzone, pws.PRMSGroundwater, pws.PRMSChannel]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dace9c93-84a5-4fc2-ad44-a415b01a36f9",
   "metadata": {},
   "source": [
    "This prompts the question, what inputs/forcing data do we need for this submodel? We can ask each individual process for its inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbc5725-799e-435c-8fdd-86bce771bdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "submodel_input_dict = {\n",
    "    pp.__name__: pp.get_inputs() for pp in submodel_processes\n",
    "}\n",
    "pprint(submodel_input_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1623d3-070b-421a-86a4-d8ba8eaad4d2",
   "metadata": {},
   "source": [
    "And which inputs are supplied by variables within this submodel? We ask each process for its variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b97c7e-7d2e-4279-a7ee-b674679a180d",
   "metadata": {},
   "outputs": [],
   "source": [
    "submodel_vars_dict = {\n",
    "    pp.__name__: pp.get_variables() for pp in submodel_processes\n",
    "}\n",
    "pprint(submodel_vars_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f527ee3-8547-4afd-92f0-4f56f206ead1",
   "metadata": {},
   "source": [
    "We consolidate inputs and variables (each over all processes) and take a set difference of inputs and variables to know what inputs/forcings we need from file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99082da-cd5d-4c98-b57c-f716ebd01f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "submodel_inputs = set([ii for tt in submodel_input_dict.values() for ii in tt])\n",
    "submodel_variables = set(\n",
    "    [ii for tt in submodel_vars_dict.values() for ii in tt]\n",
    ")\n",
    "submodel_file_inputs = tuple(submodel_inputs - submodel_variables)\n",
    "pprint(submodel_file_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f9f113-bdcf-4595-a49e-baa16a7e6205",
   "metadata": {},
   "source": [
    "And where will we get these input files? If you pay close attention, you'll notice that these files do not come with the repository. Instead they are generated when we ran the full NHM model above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b453372-b218-4936-9046-3b47cf64460b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii in submodel_file_inputs:\n",
    "    input_file = nhm_output_dir / f\"{ii}.nc\"\n",
    "    assert input_file.exists()\n",
    "    print(input_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5892326e-885e-4c03-8f29-567a4634a9d6",
   "metadata": {},
   "source": [
    "Well, that was a lot of work. But, as alluded to above, the `Model` object does the above so you dont have to. You just learned something about how the flow of information between processes is enabled by how the classes are designed and how to query the individual processes in `pywatershed`. But we could just instantiate the submodel and plot this wiring up, as we plotted the ModelGraph of the full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c0ce51-937f-4dbe-8443-1857ed1be52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = pws.parameters.PrmsParameters.load(domain_dir / \"myparam.param\")\n",
    "control = pws.Control.load(domain_dir / \"control.test\")\n",
    "control.edit_n_time_steps(90)\n",
    "control.config = control.config | {\n",
    "    \"input_dir\": nhm_output_dir,\n",
    "    \"budget_type\": \"warn\",\n",
    "    \"calc_method\": \"numba\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab0a561-8642-40a9-8638-36d380672cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "submodel = pws.Model(\n",
    "    submodel_processes,\n",
    "    control=control,\n",
    "    parameters=params,\n",
    ")\n",
    "\n",
    "pws.analysis.ModelGraph(\n",
    "    submodel,\n",
    "    hide_variables=not show_params,\n",
    "    show_params=show_params,\n",
    "    process_colors=palette,\n",
    ").SVG(verbose=True, dpi=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f7f4f3-12d4-4dda-a7c3-6e1a761b4a5a",
   "metadata": {},
   "source": [
    "Now we can initalize output and run the submodel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814491bf-c945-44ec-a733-93aff4b56b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "submodel_output_dir = output_root_dir / \"submodel\"\n",
    "submodel.initialize_netcdf(output_dir=submodel_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35002e93-b777-472e-8899-36548c1ea923",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "submodel.run(finalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa134e8e-5d93-4971-a629-3773b73ad65d",
   "metadata": {},
   "source": [
    "We'll, that saved us some time. The run is similar to before, just using fewer processes. \n",
    "\n",
    "The final time is still in memory. We can take a look at, say, recharge. Before plotting, let's take a look at the data and the metadata for recharge a bit closer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbf15be-871b-4b58-ad1f-45bfcb37fedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(pws.meta.find_variables(\"recharge\"))\n",
    "print(\n",
    "    \"PRMSSoilzone dimension names: \",\n",
    "    submodel.processes[\"PRMSSoilzone\"].dimensions,\n",
    ")\n",
    "print(\"nhru: \", submodel.processes[\"PRMSSoilzone\"].nhru)\n",
    "print(\n",
    "    \"PRMSSoilzone recharge shape: \",\n",
    "    submodel.processes[\"PRMSSoilzone\"][\"recharge\"].shape,\n",
    ")\n",
    "print(\n",
    "    \"PRMSSoilzone recharge type: \",\n",
    "    type(submodel.processes[\"PRMSSoilzone\"][\"recharge\"]),\n",
    ")\n",
    "print(\n",
    "    \"PRMSSoilzone recharge dtype: \",\n",
    "    submodel.processes[\"PRMSSoilzone\"][\"recharge\"].dtype,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266c5e9f-e58b-4d53-af55-c20f78c96b5a",
   "metadata": {},
   "source": [
    "First we access the metadata on `recharge` and we see its description, dimension, type, and units. The we look at the dimension names of the PRMSSoilzone process in whith it is found. We see the length of the `nhru` dimension and that this is the only dimension on `recharge`. We also see that `recharge` is a `numpy.ndarray` with data type `float64`.\n",
    "\n",
    "So recharge only has spatial dimension. It is written to file with each timestep (or periodically). However, the last timestep is still in memory (even though we've finalized the run) and we can visualize it. This time the data are on the unstructured/polygon grid of Hydrologic Response Units (HRUs) instead of the streamflow network plotted above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ac5832-7437-467e-901d-c40e2d9ddab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_plot = pws.analysis.process_plot.ProcessPlot(domain_gis_dir)\n",
    "proc_name = \"PRMSSoilzone\"\n",
    "var_name = \"recharge\"\n",
    "proc = submodel.processes[proc_name]\n",
    "display(proc_plot.plot(var_name, proc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c9ee58-7ed8-4540-a110-fcb7077e0e7d",
   "metadata": {},
   "source": [
    "We can easily check the results of our submodel model against our full model. This gives us an opportunity to look at the output files. We can start with recharge as our variable of interest. The model NetCDF output can be read in using `xarray` where we can see all the relevant metadata quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921eb7ef-c6e1-4ecf-b475-59d8552117b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "var = \"recharge\"\n",
    "nhm_ds = xr.open_dataset(nhm_output_dir / f\"{var}.nc\")\n",
    "sub_ds = xr.open_dataset(submodel_output_dir / f\"{var}.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e7b2e0-c9b6-4100-a328-42add2cbc4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(nhm_ds)\n",
    "display(sub_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc835b5-a677-4817-b42b-cf1c391604b6",
   "metadata": {},
   "source": [
    "If you expand the metadata (the little folded page icon on the right side of the recharge line), you get a variable description, dimension, type, and units. \n",
    "\n",
    "Now compare all output variables common to both runs, asserting that the two runs gave equal output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfeb46f-8d29-49f4-8945-f9b443831f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in submodel_variables:\n",
    "    nhm_da = xr.open_dataset(nhm_output_dir / f\"{var}.nc\")[var]\n",
    "    sub_da = xr.open_dataset(submodel_output_dir / f\"{var}.nc\")[var]\n",
    "    xr.testing.assert_allclose(nhm_da, sub_da, rtol=1.0e-9, atol=1.0e-15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad294441-cdc9-40f6-a1c7-2623174b6dd3",
   "metadata": {},
   "source": [
    "### Zooming in, in pywatershed\n",
    "\n",
    "For illustrative purposes, say you are interested some of the details of the groundwater representation in the above submodel. For a start, you can ask that class for its description. It returns a dictionary with the top-level keys: `class_name`, `inputs`, `mass_budget_terms`, `parameters`, and `variables`. For `inputs`, `parameters`, and `variables`, the corresponding value is a dictionary of metadata about the data in each of those categories. For `mass_budget_terms`, the data involved in the calculation of the budgets are summarized in terms of `inputs`, `outputs`, and `storage_changes`. It is easy to quickly get oriented to the model via this description. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ea4f8c-9371-41dd-ae44-477d73270bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(pws.PRMSGroundwater.description())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef45f88-8346-48d2-a35c-b65a71775529",
   "metadata": {},
   "source": [
    "Next, lets say we are interested in looking at certain variables in real time, as the model runs. We'll pull apart the time advance loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce075cf7-cc92-4a6c-a153-081eb19b4576",
   "metadata": {},
   "outputs": [],
   "source": [
    "submodel_output_dir = pkg_root_dir / \"model_runs/submodel\"\n",
    "params = pws.parameters.PrmsParameters.load(domain_dir / \"myparam.param\")\n",
    "control = pws.Control.load(domain_dir / \"control.test\")\n",
    "control.edit_n_time_steps(90)\n",
    "control.config = control.config | {\n",
    "    \"input_dir\": nhm_output_dir,\n",
    "    \"budget_type\": \"warn\",\n",
    "    \"calc_method\": \"numba\",\n",
    "}\n",
    "submodel = pws.Model(\n",
    "    submodel_processes,\n",
    "    control=control,\n",
    "    parameters=params,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acab1ad-c485-454f-a6e0-c4ddf6fd3bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tt in list(range(control.n_times))[0:3]:\n",
    "    print(\"time step: \", tt)\n",
    "    submodel.advance()\n",
    "    print(\n",
    "        \"timestep start time: \", control.current_time\n",
    "    )  # The daily timestep really means \"at the end of the day\"\n",
    "    submodel.calculate()\n",
    "    print(\n",
    "        \"mean gwres_stor: \",\n",
    "        submodel.processes[\"PRMSGroundwater\"].gwres_stor.mean(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4aec457-4033-428b-a43d-3210b6416f84",
   "metadata": {},
   "source": [
    "Finally, if we want mass balance at the current timestep, we can just print a process's budget. The terms in blue in the ModelGraph are those that participate in the budget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c37ab6c-f700-4746-842d-d70855f9125a",
   "metadata": {},
   "outputs": [],
   "source": [
    "submodel.processes[\"PRMSGroundwater\"].budget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636e6b8b-30e7-4fdb-a1bf-111a4547f777",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "This notebook demonstrates the capabilities of `pywatershed` to flexibly model process representations in a way that is friendly to both users and developers. Its design is mean to accelerate interaction of conceptual components implemented in Python and beyond and to advance hdyrologic modeling for novel applications.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866e53f1-0021-4b78-8b09-cd25200e371f",
   "metadata": {},
   "source": [
    "## References\n",
    "* Regan, R. S., Markstrom, S. L., Hay, L. E., Viger, R. J., Norton, P. A., Driscoll, J. M., & LaFontaine, J. H. (2018). Description of the national hydrologic model for use with the precipitation-runoff modeling system (prms) (No. 6-B9). US Geological Survey.\n",
    "* Regan, R.S., Markstrom, S.L., LaFontaine, J.H., 2022, PRMS version 5.2.1: Precipitation-Runoff Modeling System (PRMS): U.S. Geological Survey Software Release, 02/10/2022.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
