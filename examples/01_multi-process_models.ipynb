{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e142cbc-9b4b-44d1-8991-f98a57243b08",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Multi-process models in pywatershed\n",
    "\n",
    "In notebook `00_processes.ipynb`, we looked at how and individual Process representations work and are designed. In this notebook we learn how to put multiple Processes together into composite models using the `Model` class. \n",
    "\n",
    "The starting point for the development of `pywatershed` was the National Hydrologic Model (NHM, Regan et al., 2018) configuration of the Precipitation-Runoff Modeling System (PRMS, Regan et al., 2015). In this notebook, we'll first construct a full NHM configuration. The spatial domain we'll use will again be the Delaware River Basin. Once we construct the full NHM, we'll look at how we can also construct sub-models of the NHM.\n",
    "\n",
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ea5714-149a-44ed-8c66-7ef53aa2d9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto-format the code in this notebook\n",
    "%load_ext jupyter_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d5fb06-f73f-489b-9eb8-760e7f5fee57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import pathlib as pl\n",
    "from platform import processor\n",
    "from pprint import pprint\n",
    "from sys import platform\n",
    "import yaml\n",
    "\n",
    "import pydoc\n",
    "\n",
    "import hvplot.xarray  # noqa\n",
    "import numpy as np\n",
    "import pywatershed as pws\n",
    "\n",
    "# from tqdm.notebook import tqdm\n",
    "import xarray as xr\n",
    "\n",
    "from helpers import gis_files\n",
    "\n",
    "gis_files.download()  # this downloads GIS files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6a0f3d-4583-4a96-844d-bbbeb652d374",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_dir = pws.constants.__pywatershed_root__ / \"data/drb_2yr\"\n",
    "nb_output_dir = pl.Path(\"./01_multi-process_models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8896bb-96bc-4879-a2d5-38d083b33edd",
   "metadata": {},
   "source": [
    "## An NHM multi-process model\n",
    "The 8 conceptual `Process` classes that comprise the NHM are, in order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529247fe-7f0b-4783-90a4-a4ddf55c1489",
   "metadata": {},
   "outputs": [],
   "source": [
    "nhm_processes = [\n",
    "    pws.PRMSSolarGeometry,\n",
    "    pws.PRMSAtmosphere,\n",
    "    pws.PRMSCanopy,\n",
    "    pws.PRMSSnow,\n",
    "    pws.PRMSRunoff,\n",
    "    pws.PRMSSoilzone,\n",
    "    pws.PRMSGroundwater,\n",
    "    pws.PRMSChannel,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdde067-5032-443d-9c7c-6fc0340f7be8",
   "metadata": {},
   "source": [
    "We'll use this list of classes shortly to construct the NHM.\n",
    "\n",
    "A multi-process model is assembled by the `Model` class. We can take a quick look at the first 21 lines of help on `Model`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1bfbf7-00e0-4642-b7a6-dc06eee5722c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is equivalent to help() but we get the multiline string and just look at part of it\n",
    "model_help = pydoc.render_doc(pws.Model, \"Help on %s\")\n",
    "# the first 22 lines of help(pws.Model)\n",
    "print(\"\\n\".join(model_help.splitlines()[0:22]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbfb5e1-adf4-4eef-949a-90476e00b9f2",
   "metadata": {},
   "source": [
    "The `help()` mentions that there are 2 distinct ways of instantiating a `Model` class. In this notebook, we focus on the pywatershed-centric instatation and leave the PRMS-legacy instantiation to the following notebook. \n",
    "\n",
    "With the pywatershed-centric approach, the first argument is a \"model dictionary\" which does nearly all the work (the other arguments will be their default values). The `help()` describes the model dictionary and provides examples. Please use it for reference and more details. Here we'll give an extended concrete example. The `help()` also describes how a `Model` can be instantiated from a model dictionary contained in a YAML file. First we'll build a model dictionary in memory, then we'll write it out as a yaml file and instantiate our model directly from the YAML file. \n",
    "\n",
    "## Model dictionary in memory\n",
    "\n",
    "Because our (pre-existing) parameter files (which come with `pywatershed`) and our `Process` classes are consistently named, we can begin to build the model dictionary quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bd5038-e0bd-4e8e-9020-00520f18b674",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {}\n",
    "\n",
    "for proc in nhm_processes:\n",
    "    # this is the class name\n",
    "    proc_name = proc.__name__\n",
    "    # the processes can have arbitrary names in the model_dict and\n",
    "    # an instance should not have capitalized name anyway (according to\n",
    "    # python convention), so rename from the class name\n",
    "    proc_rename = \"prms_\" + proc_name[4:].lower()\n",
    "    # each process has a dictionary of information\n",
    "    model_dict[proc_rename] = {}\n",
    "    # alias to shorten lines below\n",
    "    proc_dict = model_dict[proc_rename]\n",
    "    # required key \"class\" specifys the class\n",
    "    proc_dict[\"class\"] = proc\n",
    "    # the \"parameters\" key provides an instance of Parameters\n",
    "    proc_param_file = domain_dir / f\"parameters_{proc_name}.nc\"\n",
    "    proc_dict[\"parameters\"] = pws.Parameters.from_netcdf(proc_param_file)\n",
    "    # the \"dis\" key provides the name of the discretizations\n",
    "    # which we'll supply shortly to the model dictionary\n",
    "    if proc_rename == \"prms_channel\":\n",
    "        proc_dict[\"dis\"] = \"dis_both\"\n",
    "    else:\n",
    "        proc_dict[\"dis\"] = \"dis_hru\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f649ef-2723-44b9-9c49-3f2db85a1010",
   "metadata": {},
   "source": [
    "Let's look at what we have so far in the `model_dict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57647846-7159-4405-8e46-aab00594c9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(model_dict, sort_dicts=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059641b6-8fbb-49b4-a95f-4e7fd29af3a6",
   "metadata": {},
   "source": [
    "We have given a name to each process and then supplied the class, its parameters, and its discretization for the full set of processes. Now we'll need to add the discretizations to the model dictionary. They are added at the top level and correspond to the names the processes used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c45a89-b394-4f90-af7d-7bfcf715b0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = model_dict | {\n",
    "    \"dis_hru\": pws.Parameters.from_netcdf(domain_dir / \"parameters_dis_hru.nc\"),\n",
    "    \"dis_both\": pws.Parameters.from_netcdf(domain_dir / \"parameters_dis_both.nc\"),\n",
    "}\n",
    "pprint(model_dict, sort_dicts=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75b11ab-4c2c-4671-9127-1578d8a9cfe0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "For the time being, `PRMSChannel` needs to know about both HRUs and segments, so `dis_both` is used. We plan to remove this requirement in the near future by implementing \"exchanges\" between processes into the model dictionary. Stay tuned.\n",
    "\n",
    "You may have noticed that we are missing a `Control` object to provide time information to the processes. We'll create it and we'll also create a list of the order that the processes are executed.\n",
    "\n",
    "Though we have input available to run 2 years of simulation, we'll restrict the model run to the first 6 months for demonstration purposes. (Feel free to increase this to the full 2 years available, if you like.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4e38dc-f01c-4ffc-933e-d4c2a901a1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "control = pws.Control(\n",
    "    start_time=np.datetime64(\"1979-01-01T00:00:00\"),\n",
    "    end_time=np.datetime64(\"1979-07-01T00:00:00\"),\n",
    "    time_step=np.timedelta64(24, \"h\"),\n",
    "    options={\n",
    "        \"input_dir\": domain_dir,\n",
    "        \"budget_type\": None,\n",
    "        \"netcdf_output_dir\": nb_output_dir / \"nhm_memory\",\n",
    "        \"init_vars_from_file\": 0,\n",
    "        \"dprst_flag\": True,\n",
    "    },\n",
    ")\n",
    "model_order = [\"prms_\" + proc.__name__[4:].lower() for proc in nhm_processes]\n",
    "model_dict = model_dict | {\"control\": control, \"model_order\": model_order}\n",
    "pprint(model_dict, sort_dicts=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546a7ac8-c23e-4632-b655-6dc8e884172d",
   "metadata": {},
   "source": [
    "The `model_dict` now specifies a complete model built from multiple processes. They way these processes are connected can be figured out by the `Model` class, because each process fully describes itself (as we saw in the previous notebook). If we instantiate a model from this `model_dict`,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ba1ed4-fb1b-4fa1-bd38-f47113bdaa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mem = pws.Model(model_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7260278e-91ff-47af-a2c1-b9c02e8beaa1",
   "metadata": {},
   "source": [
    "we can examine how the `Processes` are all connected using the `ModelGraph` class. We'll bring in the default color scheme for NHM `Processes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c81929f-76d8-470f-9976-7d23f8c1af91",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "palette = pws.analysis.utils.colorbrewer.nhm_process_colors(model_mem)\n",
    "pws.analysis.utils.colorbrewer.jupyter_palette(palette)\n",
    "show_params = not (platform == \"darwin\" and processor() == \"arm\")\n",
    "try:\n",
    "    pws.analysis.ModelGraph(\n",
    "        model_mem,\n",
    "        hide_variables=False,\n",
    "        process_colors=palette,\n",
    "        show_params=show_params,\n",
    "    ).SVG(verbose=True, dpi=48)\n",
    "except:\n",
    "    print(\"In some cases, dot fails on Mac ARM machines.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc28c9b-8d33-479f-b4f8-b8985d86d9c8",
   "metadata": {},
   "source": [
    "Now we'll initialize NetCDF output and run the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707d6a26-3aa0-4064-b1ea-117fd2bff6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_mem.run(finalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5cd00e-46a5-4778-abc5-f3566f7fe74a",
   "metadata": {},
   "source": [
    "We'll take a look at the outputs after we see how to implement this model using a YAML file on disk. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2728d285-f0cd-43b0-96ff-9d019c7e0fda",
   "metadata": {},
   "source": [
    "## Model dictionary yaml file\n",
    "It may be preferable to have a model dictionary encoded in YAML file in many situations. Let's do that. Necessarily, the contents of the YAML file will look different than above where we had the contents of the model dictionary in memory.\n",
    "\n",
    "First we'll need to write the control instance as a YAML file. To do that we need a serializable dictionary in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbdca24-7347-4ae7-91de-cee896461e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_dir = pl.Path(nb_output_dir / \"nhm_yaml\")\n",
    "run_dir.mkdir(exist_ok=True)\n",
    "control_dict = control.options | {\n",
    "    \"start_time\": str(control.start_time),\n",
    "    \"end_time\": str(control.end_time),\n",
    "    \"time_step\": str(control.time_step)[0:2],\n",
    "    \"time_step_units\": str(control.time_step)[3:4],\n",
    "    \"netcdf_output_dir\": run_dir,\n",
    "}\n",
    "\n",
    "pprint(control_dict, sort_dicts=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aaa0705-ac8d-455d-9f56-8ba7b7353c44",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "We add the option `netcdf_output_dir` to the control since we assume we wont be able to do so at run time. Note that this option and the `input_dir` option are `pathlib.Path` objects. These are not what we want to write to file. We want their string version. We could do `str()` on each one by hand, but it will be more handy to write a small, recursive function to do this on a supplied dictionary since this will be a recurring task with the model dictionary we will create after the control YAML file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e907ed-29e1-44f0-9960-98f70db6a578",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_pl_to_str(the_dict):\n",
    "    for key, val in the_dict.items():\n",
    "        if isinstance(val, dict):\n",
    "            the_dict[key] = dict_pl_to_str(val)\n",
    "        elif isinstance(val, pl.Path):\n",
    "            the_dict[key] = str(val)\n",
    "\n",
    "    return the_dict\n",
    "\n",
    "\n",
    "control_dict = dict_pl_to_str(control_dict)\n",
    "pprint(control_dict, sort_dicts=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51538c2-6431-4866-adff-eb9e15acb15c",
   "metadata": {},
   "source": [
    "Now we'll create the model dictionary. For the `control` field, we'll need the path to the YAML file to which we will write the information above. For discretization fields, we'll pass paths to NetCDF files instead of instantiated `Parameter` objects. For `model_order` we can supply the same list we used above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe43720-4df3-4b34-b964-ba2af481c9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_yaml_file = run_dir / \"control.yml\"\n",
    "model_dict = {\n",
    "    \"control\": control_yaml_file.resolve(),\n",
    "    \"dis_hru\": domain_dir / \"parameters_dis_hru.nc\",\n",
    "    \"dis_both\": domain_dir / \"parameters_dis_both.nc\",\n",
    "    \"model_order\": model_order,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e563b5-d540-4055-8e5e-9b8cb920ddda",
   "metadata": {},
   "source": [
    "Now, for each process, we'll use an arbitray name. Then we'll supply the class name (which can be imported from `pws` at the top level), the path to its NetCDF parameter file, and the name of its required discretization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78b326c-105b-4a70-a374-3da019993b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for proc in nhm_processes:\n",
    "    proc_name = proc.__name__\n",
    "    proc_rename = \"prms_\" + proc_name[4:].lower()\n",
    "    model_dict[proc_rename] = {}\n",
    "    proc_dict = model_dict[proc_rename]\n",
    "    proc_dict[\"class\"] = proc_name\n",
    "    proc_param_file = domain_dir / f\"parameters_{proc_name}.nc\"\n",
    "    proc_dict[\"parameters\"] = proc_param_file\n",
    "    if proc_rename == \"prms_channel\":\n",
    "        proc_dict[\"dis\"] = \"dis_both\"\n",
    "    else:\n",
    "        proc_dict[\"dis\"] = \"dis_hru\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1162367-7898-4a6a-bf05-3cf79fcfa99a",
   "metadata": {},
   "source": [
    "Before looking at it, we'll convert `Path` objects to strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d939a7-4842-4329-b278-d0861556ea50",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = dict_pl_to_str(model_dict)\n",
    "pprint(model_dict, sort_dicts=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fddaf2-d3d0-46b2-b463-9708c03ab6ac",
   "metadata": {},
   "source": [
    "A note on paths in the yaml file. Because we are using files in two different locations which are not easily described relative to the location of yaml file, we are using absolute paths. However, one can also describe all paths relative to the location of the yaml file if that is more suitable to your purposes. \n",
    "\n",
    "Finally, we have the control and model dictionaries ready to write to yaml."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caddaf9-37f3-40a3-b054-6a275f0f3e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict_yaml_file = run_dir / \"model_dict.yml\"\n",
    "# the control yaml file was given above and is in the model_dict\n",
    "dump_dict = {control_yaml_file: control_dict, model_dict_yaml_file: model_dict}\n",
    "for key, val in dump_dict.items():\n",
    "    with open(key, \"w\") as file:\n",
    "        documents = yaml.dump(val, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4230eeac-3feb-4534-896f-442cc9e85451",
   "metadata": {},
   "source": [
    "We'll use a little magics to directly examine the written YAML files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc2a81f-bac2-47a6-b161-07088c96e185",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat 01_multi-process_models/nhm_yaml/control.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8901961b-527a-419d-82fc-a3cb16af04ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat 01_multi-process_models/nhm_yaml/model_dict.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ec433f-908e-4519-9abf-9e5527d1844c",
   "metadata": {},
   "source": [
    "Now we can create a `Model` from these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7891df03-efd2-44b5-9a9e-3ba61a72fa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_yml = pws.Model.from_yml(model_dict_yaml_file)\n",
    "model_yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8effb95a-3ead-4d67-a2e6-02816e4f7335",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_params = not (platform == \"darwin\" and processor() == \"arm\")\n",
    "try:\n",
    "    pws.analysis.ModelGraph(\n",
    "        model_yml,\n",
    "        hide_variables=False,\n",
    "        process_colors=palette,\n",
    "        show_params=show_params,\n",
    "    ).SVG(verbose=True, dpi=48)\n",
    "except:\n",
    "    print(\"In some cases, dot fails on Mac ARM machines.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78d3fa6-722b-404f-a60f-6778dbb8109a",
   "metadata": {},
   "source": [
    "That looks identical to the `model_mem` model constructed previously. Let's run the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842a2b97-fb2e-4413-8d32-c8aa08b31dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_yml.run()\n",
    "model_yml.finalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e68f6e7-f707-4012-a74b-fc85020ff86a",
   "metadata": {},
   "source": [
    "## Compare the outputs of the two models\n",
    "Let's see that these constructed and executed the same models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f305fd-0cfa-4b89-a1fb-c32e72490b8a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mem_out_dir = nb_output_dir / \"nhm_memory\"\n",
    "yml_out_dir = nb_output_dir / \"nhm_yaml\"\n",
    "mem_files = sorted(mem_out_dir.glob(\"*.nc\"))\n",
    "yml_files = sorted(yml_out_dir.glob(\"*.nc\"))\n",
    "# We get all the same output files\n",
    "assert set([ff.name for ff in mem_files]) == set([ff.name for ff in yml_files])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1024fe62-ac86-4171-8c45-018f1ac35902",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Now compare the values of all variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0d920e-0227-45ba-92c6-8ffacab9ca3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for mf, yf in zip(mem_files, yml_files):\n",
    "    var = mf.with_suffix(\"\").name\n",
    "    # print(var)\n",
    "    mda = xr.open_dataset(mf)[var]\n",
    "    yda = xr.open_dataset(yf)[var]\n",
    "    xr.testing.assert_equal(mda, yda)\n",
    "    mda.close()\n",
    "    yda.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3078346e-8b0f-48f0-b7a9-4200714b403b",
   "metadata": {},
   "source": [
    "We'll plot the last variable in the loop, `unused_potet`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7213a424-db51-4f36-974f-56c2b4ea049d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mda.hvplot(groupby=\"nhm_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45454b13-020b-4867-bd34-21c8cdb00834",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "proc_plot = pws.analysis.process_plot.ProcessPlot(gis_files.gis_dir / \"drb_2yr\")\n",
    "proc_plot.plot_hru_var(\n",
    "    var_name=var,\n",
    "    process=pws.PRMSAtmosphere,\n",
    "    data=mda.mean(dim=\"time\"),\n",
    "    data_units=mda.attrs[\"units\"],\n",
    "    nhm_id=mda[\"nhm_id\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae49ade-3c8b-4fe8-af5a-9497f856151b",
   "metadata": {},
   "source": [
    "## Reduce model output to disk\n",
    "It's worth noting that quite a lot of output was written and that in many cases the amount of output can be reduced in favor of imporving/reducing model run time. Let's show how easily that can be done.\n",
    "\n",
    "Because we want to reuse the above control dict and model dict for the submodel demonstration in the next section, we'll deep copy them for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f00265-1d4c-462e-9be1-bf8cd82261ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_dict_copy = deepcopy(control_dict)\n",
    "model_dict_copy = deepcopy(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284d8184-9837-4c01-97db-ea1e53f956ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_dir = pl.Path(nb_output_dir / \"yml_less_output\").resolve()\n",
    "run_dir.mkdir(exist_ok=True)\n",
    "\n",
    "control_dict_copy[\"netcdf_output_dir\"] = str(run_dir.resolve())\n",
    "control_yaml_file = run_dir / \"control.yml\"\n",
    "control_dict_copy[\"netcdf_output_var_names\"] = [\n",
    "    var\n",
    "    for ll in [\n",
    "        pws.PRMSGroundwater.get_variables(),\n",
    "        pws.PRMSChannel.get_variables(),\n",
    "    ]\n",
    "    for var in ll\n",
    "]\n",
    "pprint(control_dict_copy, sort_dicts=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eea5150-fe9a-4c6d-95bd-9ed80c061457",
   "metadata": {},
   "source": [
    "Now we will use the existing `model_dict` in memory, tayloring to the above and just keeping the processes of interest in the submodel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02da6d0-e397-4a88-aff5-c4c83e8f1a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict_copy[\"control\"] = str(control_yaml_file)\n",
    "model_dict_yaml_file = run_dir / \"model_dict.yml\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020a6a11-c36b-4f4b-bca5-879105083fe5",
   "metadata": {},
   "source": [
    "Now we write both the control and model dictionary to yaml files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051df876-2e37-4857-976b-ef6c0d586985",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_dict = {\n",
    "    control_yaml_file: control_dict_copy,\n",
    "    model_dict_yaml_file: model_dict_copy,\n",
    "}\n",
    "for key, val in dump_dict.items():\n",
    "    with open(key, \"w\") as file:\n",
    "        documents = yaml.dump(val, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a275786-239b-4f8a-9643-e65ad6ef9dc8",
   "metadata": {},
   "source": [
    "And finally we instantiate the submodel from the model dictionary yaml file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc3f3f6-3d5c-4e4f-84c4-5203db595dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "submodel = pws.Model.from_yml(model_dict_yaml_file)\n",
    "submodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f62c723-28ae-4db0-939e-03e477c6f2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "submodel.run(finalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b420c326-be66-41df-82f1-3c332c1f85ea",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Reducing the output significantly reduced the time, in this case (on my machine) from 25s to 15s, or about 60%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e3d4c4-8bea-44e2-be76-b274bf04c3d7",
   "metadata": {},
   "source": [
    "## NHM Submodel for the Delaware River Basin \n",
    "In many cases, running the full NHM model may not be necessary and it may be advantageous to just run some of the processes in it. Suppose you wanted to change parameters or model process representation in the PRMSSoilzone to better predict streamflow. As the model is 1-way coupled, you can simply run a submodel starting with PRMSSoilzone and running through PRMSChannel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9e1807-bed4-4c52-99e0-b5a58e63eaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "submodel_processes = [pws.PRMSSoilzone, pws.PRMSGroundwater, pws.PRMSChannel]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd91adfe-a875-4873-b8ba-85778a7cb651",
   "metadata": {},
   "source": [
    "This prompts the question, what inputs/forcing data do we need for this submodel? We can ask each individual process for its inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbc5725-799e-435c-8fdd-86bce771bdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "submodel_input_dict = {pp.__name__: pp.get_inputs() for pp in submodel_processes}\n",
    "pprint(submodel_input_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002e6e21-c16d-404f-9910-e764f18215b9",
   "metadata": {},
   "source": [
    "And which inputs are supplied by variables within this submodel? We ask each process for its variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b97c7e-7d2e-4279-a7ee-b674679a180d",
   "metadata": {},
   "outputs": [],
   "source": [
    "submodel_vars_dict = {pp.__name__: pp.get_variables() for pp in submodel_processes}\n",
    "pprint(submodel_vars_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed052fe-a46d-4470-8df5-bf03e2ecbb7d",
   "metadata": {},
   "source": [
    "We consolidate inputs and variables (each over all processes) and take a set difference of inputs and variables to know what inputs/forcings we need from file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99082da-cd5d-4c98-b57c-f716ebd01f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "submodel_inputs = set([ii for tt in submodel_input_dict.values() for ii in tt])\n",
    "submodel_variables = set([ii for tt in submodel_vars_dict.values() for ii in tt])\n",
    "submodel_file_inputs = tuple(submodel_inputs - submodel_variables)\n",
    "pprint(submodel_file_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c789efb-b982-48d2-91cd-c555820fccb3",
   "metadata": {},
   "source": [
    "And where will we get these input files? If you pay close attention, you'll notice that these files do not come with the repository. Instead they are generated when we ran the full NHM model above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b453372-b218-4936-9046-3b47cf64460b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "yml_output_dir = pl.Path(control_dict[\"netcdf_output_dir\"])\n",
    "for ii in submodel_file_inputs:\n",
    "    input_file = yml_output_dir / f\"{ii}.nc\"\n",
    "    assert input_file.exists()\n",
    "    print(input_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e933dcd1-ce14-4113-b829-693e830e9172",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Well, that was a lot of work. But, as alluded to above, the `Model` object does the above so you dont have to. You just learned something about how the flow of information between processes is enabled by the design and how one can query individual processes in `pywatershed`. But we could instantiate the submodel and plot this wiring up, just as we plotted the `ModelGraph` of the full model. We'll create the submodel in a new `run_dir` and we'll use outputs from the full model above as inputs to this submodel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ef25e0-7d30-446a-acfd-28af3d6e5ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_dir = pl.Path(nb_output_dir / \"nhm_sub\").resolve()\n",
    "run_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# key that inputs exist from previous full-model run\n",
    "control_dict[\"input_dir\"] = str(yml_output_dir.resolve())\n",
    "control_dict[\"netcdf_output_dir\"] = str(run_dir.resolve())\n",
    "control_yaml_file = run_dir / \"control.yml\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b81d5f5-e733-4347-b898-5e1a23f36a06",
   "metadata": {},
   "source": [
    "Now we will use the existing `model_dict` in memory, tayloring to the above and just keeping the processes of interest in the submodel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd108f6-aa1f-4f59-90c5-a8fe6018c90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict[\"control\"] = str(control_yaml_file)\n",
    "model_dict_yaml_file = run_dir / \"model_dict.yml\"\n",
    "keep_procs = [\"prms_soilzone\", \"prms_groundwater\", \"prms_channel\"]\n",
    "model_dict[\"model_order\"] = keep_procs\n",
    "for kk in list(model_dict.keys()):\n",
    "    if isinstance(model_dict[kk], dict) and kk not in keep_procs:\n",
    "        del model_dict[kk]\n",
    "pprint(control_dict, sort_dicts=False)\n",
    "pprint(model_dict, sort_dicts=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fdef0c-d77b-45bd-a6a2-cc227acbfae0",
   "metadata": {},
   "source": [
    "Now we write both the control and model dictionary to yaml files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4000d71a-3f58-46da-98fa-3c573b861b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_dict = {control_yaml_file: control_dict, model_dict_yaml_file: model_dict}\n",
    "for key, val in dump_dict.items():\n",
    "    with open(key, \"w\") as file:\n",
    "        documents = yaml.dump(val, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f889c7-3763-4ed2-b93f-e3d6c2a3f67e",
   "metadata": {},
   "source": [
    "And finally we instantiate the submodel from the model dictionary yaml file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6570af-c1ae-4d1f-ad7b-1b32427fc647",
   "metadata": {},
   "outputs": [],
   "source": [
    "submodel = pws.Model.from_yml(model_dict_yaml_file)\n",
    "submodel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22155c95-1de6-45bf-9dea-f562df9fdeaf",
   "metadata": {},
   "source": [
    "Now to look at the `ModelGraph` for the submodel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cae1a7a-e66e-4816-942a-201edab68b72",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_params = not (platform == \"darwin\" and processor() == \"arm\")\n",
    "try:\n",
    "    pws.analysis.ModelGraph(\n",
    "        submodel,\n",
    "        hide_variables=False,\n",
    "        process_colors=palette,\n",
    "        show_params=show_params,\n",
    "    ).SVG(verbose=True, dpi=48)\n",
    "except:\n",
    "    print(\"In some cases, dot fails on Mac ARM machines.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f9c997-9937-4207-9ca9-102f2b34f499",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Note that the required inputs to the submodel are quire different and rely on the existence of these files having already been output by the full model. \n",
    "\n",
    "Now we can initalize output and run the submodel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35002e93-b777-472e-8899-36548c1ea923",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "submodel.run(finalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c787a163-c4dd-4826-b0f2-73e1b006c081",
   "metadata": {},
   "source": [
    "We'll, that saved us some time. The run is similar to before, just using fewer processes. \n",
    "\n",
    "The final time is still in memory. We can take a look at, say, recharge. Before plotting, let's take a look at the data and the metadata for recharge a bit closer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbf15be-871b-4b58-ad1f-45bfcb37fedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(pws.meta.find_variables(\"recharge\"))\n",
    "print(\n",
    "    \"PRMSSoilzone dimension names: \",\n",
    "    submodel.processes[\"prms_soilzone\"].dimensions,\n",
    ")\n",
    "print(\"nhru: \", submodel.processes[\"prms_soilzone\"].nhru)\n",
    "print(\n",
    "    \"PRMSSoilzone recharge shape: \",\n",
    "    submodel.processes[\"prms_soilzone\"][\"recharge\"].shape,\n",
    ")\n",
    "print(\n",
    "    \"PRMSSoilzone recharge type: \",\n",
    "    type(submodel.processes[\"prms_soilzone\"][\"recharge\"]),\n",
    ")\n",
    "print(\n",
    "    \"PRMSSoilzone recharge dtype: \",\n",
    "    submodel.processes[\"prms_soilzone\"][\"recharge\"].dtype,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af149db1-970c-47d2-b9dd-0d0b21f80810",
   "metadata": {},
   "source": [
    "First we access the metadata on `recharge` and we see its description, dimension, type, and units. The we look at the dimension names of the PRMSSoilzone process in whith it is found. We see the length of the `nhru` dimension and that this is the only dimension on `recharge`. We also see that `recharge` is a `numpy.ndarray` with data type `float64`.\n",
    "\n",
    "So recharge only has spatial dimension. It is written to file with each timestep (or periodically). However, the last timestep is still in memory (even though we've finalized the run) and we can visualize it. This time the data are on the unstructured/polygon grid of Hydrologic Response Units (HRUs) instead of the streamflow network plotted above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ac5832-7437-467e-901d-c40e2d9ddab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_plot = pws.analysis.process_plot.ProcessPlot(gis_files.gis_dir / \"drb_2yr\")\n",
    "proc_name = \"prms_soilzone\"\n",
    "var_name = \"ssr_to_gw\"\n",
    "proc = submodel.processes[proc_name]\n",
    "display(proc_plot.plot(var_name, proc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13500c68-c363-4f8b-8db0-2c069aa4c5d7",
   "metadata": {},
   "source": [
    "We can easily check the results of our submodel model against our full model. This gives us an opportunity to look at the output files. We can start with recharge as our variable of interest. The model NetCDF output can be read in using `xarray` where we can see all the relevant metadata quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921eb7ef-c6e1-4ecf-b475-59d8552117b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "var = \"recharge\"\n",
    "nhm_ds = xr.open_dataset(yml_output_dir / f\"{var}.nc\")\n",
    "sub_ds = xr.open_dataset(run_dir / f\"{var}.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e7b2e0-c9b6-4100-a328-42add2cbc4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(nhm_ds)\n",
    "display(sub_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f317363-c765-4dda-a972-92f5265abd47",
   "metadata": {},
   "source": [
    "If you expand the metadata (the little folded page icon on the right side of the recharge line), you get a variable description, dimension, type, and units. \n",
    "\n",
    "Now compare all output variables common to both runs, asserting that the two runs gave equal output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfeb46f-8d29-49f4-8945-f9b443831f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in submodel_variables:\n",
    "    nhm_da = xr.open_dataset(yml_output_dir / f\"{var}.nc\")[var]\n",
    "    sub_da = xr.open_dataset(run_dir / f\"{var}.nc\")[var]\n",
    "    xr.testing.assert_equal(nhm_da, sub_da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7ed710-d23a-4c80-8b28-0a1612a636b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# var_name = \"dprst_seep_hru\"\n",
    "nhm_da = xr.open_dataset(yml_output_dir / f\"{var_name}.nc\")[var_name]\n",
    "sub_da = xr.open_dataset(run_dir / f\"{var_name}.nc\")[var_name]\n",
    "scat = xr.merge(\n",
    "    [nhm_da.rename(f\"{var_name}_yaml\"), sub_da.rename(f\"{var_name}_subset\")]\n",
    ")\n",
    "\n",
    "display(\n",
    "    scat.hvplot(x=f\"{var_name}_yaml\", y=f\"{var_name}_subset\", groupby=\"nhm_id\").opts(\n",
    "        data_aspect=1\n",
    "    )\n",
    ")\n",
    "\n",
    "scat.hvplot(y=f\"{var_name}_subset\", groupby=\"nhm_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91401478-34ab-4277-804e-6ee2d4403d52",
   "metadata": {},
   "source": [
    "## References\n",
    "* Regan, R. S., Markstrom, S. L., Hay, L. E., Viger, R. J., Norton, P. A., Driscoll, J. M., & LaFontaine, J. H. (2018). Description of the national hydrologic model for use with the precipitation-runoff modeling system (prms) (No. 6-B9). US Geological Survey.\n",
    "* Regan, R.S., Markstrom, S.L., LaFontaine, J.H., 2022, PRMS version 5.2.1: Precipitation-Runoff Modeling System (PRMS): U.S. Geological Survey Software Release, 02/10/2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4dbac1-046f-4354-94e6-2610269c2de4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pws2] *",
   "language": "python",
   "name": "conda-env-pws2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
